{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c21a0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 12:23:29.024891: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cmlscratch/sriramb/anaconda3/envs/robustlsm/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2022-02-07 12:23:29.024925: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import cvlib as cv\n",
    "import torch\n",
    "from torchvision import models, transforms as T, datasets\n",
    "from cvlib.object_detection import draw_bbox\n",
    "import torchvision\n",
    "# from automold import add_rain, add_snow, add_fog\n",
    "import clip\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ec98194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten(lol):\n",
    "    all_l = []\n",
    "    for l in lol:\n",
    "        all_l.extend(l)\n",
    "    return all_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16852caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import deplacy\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_sentence_frags(text):\n",
    "    frags = []\n",
    "    for tok in en(text):\n",
    "        if tok.dep_ == 'prep':\n",
    "            frags.append(' '.join([str(w) for w in tok.subtree]))\n",
    "    return frags\n",
    "\n",
    "def drop_frags(text):\n",
    "    frags = get_sentence_frags(text)\n",
    "    tr_texts = [text]\n",
    "    tr_texts.extend([text.replace(frag, '') for frag in frags])\n",
    "    tr_texts = [t for t in tr_texts if len(t)/len(text) > 0.4]\n",
    "    return tr_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44fd6f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man working on his laptop while a woman reads something on his desk and another eats in the background in the rain',\n",
       " 'A man working  while a woman reads something on his desk and another eats in the background in the rain',\n",
       " 'A man working on his laptop while a woman reads something  another eats in the background in the rain',\n",
       " 'A man working on his laptop while a woman reads something on his desk and another eats ',\n",
       " 'A man working on his laptop while a woman reads something on his desk and another eats in the background ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_frags('A man working on his laptop while a woman reads something on his desk and another eats in the background in the rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be349f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "def indent(f):\n",
    "    def closure(*args):\n",
    "        old = builtins.print\n",
    "        builtins.print = lambda x, *args, **kwargs:  old(\"\\t>\", x, *args, **kwargs)\n",
    "        a = f(*args)\n",
    "        builtins.print = old\n",
    "        return a\n",
    "    return closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca9fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "BICUBIC = torchvision.transforms.functional.InterpolationMode('bicubic')\n",
    "def size_info_1(x):\n",
    "    print(\"1\", x)\n",
    "    return x\n",
    "def size_info_2(x):\n",
    "    print(\"2\", x)\n",
    "    return x\n",
    "transform = T.Compose([\n",
    "                    T.Resize(224, interpolation=BICUBIC, max_size=None, antialias=None),\n",
    "#                     size_info_1,\n",
    "                    T.CenterCrop(size=(224, 224)),\n",
    "#                     size_info_2,\n",
    "                    T.ToTensor()\n",
    "                    ])\n",
    "train_cap = datasets.CocoCaptions(root = '/fs/cml-datasets/coco/images/train2017',\n",
    "                        annFile = '/fs/cml-datasets/coco/annotations/captions_train2017.json',\n",
    "                        transform=transform\n",
    "                        )#)\n",
    "test_cap = datasets.CocoCaptions(root = '/fs/cml-datasets/coco/images/val2017',\n",
    "                        annFile = '/fs/cml-datasets/coco/annotations/captions_val2017.json',\n",
    "                        transform=transform\n",
    "                        )#)\n",
    "\n",
    "batch_size=128\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_cap, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_cap, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2499a020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12e0938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(model, imgs, texts):\n",
    "    with torch.no_grad():\n",
    "        normalize = T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        text_tokens = clip.tokenize(texts).to(device)\n",
    "#         processed_imgs = torch.stack([normalize(img).to(device) for img in imgs])    \n",
    "        imgs = normalize(imgs).to(device)\n",
    "        similarities, _ = model(imgs, text_tokens)\n",
    "    return similarities.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0507ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleCLIP(torch.nn.Module):\n",
    "    def __init__(self, model, preprocess, device):\n",
    "        super(EnsembleCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        self.device = torch.device(device)\n",
    "        self.normalize = T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "    def forward(self, imgs, texts):\n",
    "        tr_texts = [drop_frags(t) for t in texts]       \n",
    "        print(tr_texts[:3])\n",
    "        num_samples = [len(t) for t in tr_texts]\n",
    "        tr_texts = [\"a photo of \"+t for t in flatten(tr_texts)]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize(tr_texts).to(self.device)\n",
    "    #         processed_imgs = torch.stack([normalize(img).to(device) for img in imgs])    \n",
    "            imgs = self.normalize(imgs).to(device)\n",
    "            similarities, _ = model(imgs, text_tokens)\n",
    "        similarities = torch.split(similarities,num_samples,dim=1)\n",
    "        similarities = torch.stack([a.mean(-1) for a in similarities], dim=-1)\n",
    "        print(similarities.shape)\n",
    "        return similarities.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7634caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalCLIP(torch.nn.Module):\n",
    "    def __init__(self, model, preprocess, device):\n",
    "        super(NormalCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        self.device = torch.device(device)\n",
    "        self.normalize = T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "    def forward(self, imgs, texts):\n",
    "        texts = [\"a photo of \"+t for t in texts]\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize(texts).to(device)\n",
    "    #         processed_imgs = torch.stack([normalize(img).to(device) for img in imgs])    \n",
    "            imgs = self.normalize(imgs).to(self.device)\n",
    "            similarities, _ = model(imgs, text_tokens)\n",
    "        return similarities.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92480e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, names):\n",
    "    for imgs, texts in test_loader:\n",
    "        imgs = torch.stack(imgs)\n",
    "        texts = [t[0] for t in texts]\n",
    "        for model, name in zip(models, names):\n",
    "            a = model(imgs, texts)\n",
    "            print(f\"Clean accuracy {name}: \",float((a.argmax(-1) == torch.arange(batch_size)).float().mean()))\n",
    "        print('\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "235a797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_clip = EnsembleCLIP(model, preprocess, device)\n",
    "normal_clip = NormalCLIP(model, preprocess, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10ee6402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean accuracy normal:  0.78125\n",
      "[['a large air plane flying thru the air', 'a large air plane flying '], ['A man riding a skateboard up a flight of steps.', 'A man riding a skateboard up a flight .'], ['a banana is laying on a small plate', 'a banana is laying ']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7578125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.7890625\n",
      "[['Baseball team holding batting practice on the field', 'Baseball team holding batting practice '], ['A crowded kitchen that is cluttered and messy.'], ['The woman holding an umbrella smiles on the narrow street beside the sidewalk.', 'The woman holding an umbrella smiles .', 'The woman holding an umbrella smiles on the narrow street .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.796875\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.765625\n",
      "[['A white meta bench next to a patch of grass.', 'A white meta bench next .', 'A white meta bench next to a patch .'], ['A group of people is standing outside of a tram.', 'A group  is standing outside of a tram.', 'A group of people is standing outside .'], ['A baby girl standing in a shopping cart holding an umbrella.']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.75\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.765625\n",
      "[['A group of large passenger jets parked in front of an airport.', 'A group  parked in front of an airport.', 'A group of large passenger jets parked .', 'A group of large passenger jets parked in front .'], ['A woman wearing a blue coat holds a furry animal in her arms', 'A woman wearing a blue coat holds a furry animal '], ['A couple of men standing on either side of a surfboard.', 'A couple of men standing .', 'A couple of men standing on either side .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.78125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.828125\n",
      "[['There is a stop sign outside of a bus window.', 'There is a stop sign outside .'], ['A laptop computer sitting on top of a wooden table.', 'A laptop computer sitting .', 'A laptop computer sitting on top .'], ['Two beautiful young women baking a turkey in a pan..', 'Two beautiful young women baking a turkey ..']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.8046875\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.8046875\n",
      "[['A corner street sign with a tow sign and a art piece', 'A corner street sign '], ['a group of young men are riding their motocycles down the street', 'a group  are riding their motocycles down the street', 'a group of young men are riding their motocycles '], ['A grey and white cat laying in window sill next to a curtain.', 'A grey and white cat laying  sill next to a curtain.', 'A grey and white cat laying in window sill next .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.765625\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.8203125\n",
      "[['Two people are riding elephants beside some trees.', 'Two people are riding elephants .'], ['The young girl wearing red and black rides atop the white pony.', 'The young girl wearing red and black rides .'], ['People inside a commuter train, with their luggage and bicycles.', 'People , with their luggage and bicycles.', 'People inside a commuter train, .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.8203125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.84375\n",
      "[['A white refrigerator freezer covered in magnets and pictures.', 'A white refrigerator freezer covered .'], ['A person makes a sandwich on a paper plate.', 'A person makes a sandwich .'], ['A few men working on moving luggage at an airport', 'A few men working on moving luggage ']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7734375\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.796875\n",
      "[['A family outside in a yard playing frisbee with their dog. ', 'A family outside in a yard playing frisbee . '], ['A person standing in shore of beach with a frisbee in the sky.', 'A person standing  with a frisbee in the sky.', 'A person standing in shore  with a frisbee in the sky.', 'A person standing in shore of beach .', 'A person standing in shore of beach with a frisbee .'], ['This is a close up photo of a street sign of a cross street.', 'This is a close up photo .', 'This is a close up photo of a street sign .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7734375\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.8046875\n",
      "[['The black street pole has three different signs on it. ', 'The black street pole has three different signs . '], ['Two brown ponies in grassy field next to a fence.', 'Two brown ponies  next to a fence.', 'Two brown ponies in grassy field next .'], ['A large slice of pizza with cheese and marinara sauce on a plate. ', 'A large slice of pizza with cheese and marinara sauce . ']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.8046875\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.828125\n",
      "[['A woman and a man playing a video game in the living room', 'A woman and a man playing a video game '], ['A box contains six donuts with varying types of glazes and toppings.', 'A box contains six donuts with varying types .'], ['A living room with hard wood floors and a book shelf.']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7890625\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.7421875\n",
      "[['A pregnant with writing on her stomach while holding a brown teddy bear.', 'A pregnant  while holding a brown teddy bear.', 'A pregnant with writing  while holding a brown teddy bear.'], ['An older picture of a large kitchen with white appliances.', 'An older picture of a large kitchen .'], ['A zebra all by itself in the green forest.', 'A zebra  in the green forest.', 'A zebra all by itself .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.78125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.8046875\n",
      "[['A bird that is flying over the sand.', 'A bird that is flying .'], ['A street sign with six different locations on it.', 'A street sign with six different locations .'], ['A large long train with man inside on the track.', 'A large long train  on the track.', 'A large long train with man inside .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7890625\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.7734375\n",
      "[[\"A large elephant scratching it's back on the side of a pole.\", \"A large elephant scratching it's back .\", \"A large elephant scratching it's back on the side .\"], ['A double city bus is pulled up to a bus stop.', 'A double city bus is pulled up .'], ['A toddler celebrates his birthday with a cupcake.', 'A toddler celebrates his birthday .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7890625\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.7421875\n",
      "[['A bride and groom cutting their wedding cake.'], ['Several men are playing sports on a field near some trees, wall, bus, and several buildings in the background.', 'Several men are playing sports on a field near some trees, wall, bus, and several buildings in the background.', 'Several men are playing sports on a field near some trees, wall, bus, and several buildings in the background.', 'Several men are playing sports on a field near some trees, wall, bus, and several buildings .'], ['A woman holding a tennis racquet on a  tennis court.', 'A woman holding a tennis racquet on a  tennis court.']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7578125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.828125\n",
      "[['a white boat some green hills and water'], ['A green traffic light suspended above a street.', 'A green traffic light suspended .'], ['A truck that is sitting in the street.', 'A truck that is sitting .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.8203125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.7578125\n",
      "[[\"A large colorful truck with a with a wooden building on it's back.\", \"A large colorful truck   wooden building on it's back.\", \"A large colorful truck with a 's back.\", \"A large colorful truck with a with a wooden building 's back.\"], ['pizza sitting on stack of plates on wood background', 'pizza sitting on stack ', 'pizza sitting on stack of plates '], ['A picture of a recently married couple displayed behind glass.', 'A picture of a recently married couple displayed .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.765625\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.859375\n",
      "[['A man serving a tennis ball on top of a tennis court.', 'A man serving a tennis ball .', 'A man serving a tennis ball on top .'], ['The young boy just finished eating the banana.'], ['Large group of people surrounding a truck on a mountain. ', 'Large group of people surrounding a truck . ']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7890625\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.796875\n",
      "[['a white dog is sitting in the back of a truck and a mattress', 'a white dog is sitting in the back  and a mattress'], ['A group of people looking at some kind of show or exhibit ', 'A group of people looking  ', 'A group of people looking at some kind  '], ['Large assortment of small and large vases displayed outdoors.', 'Large assortment  displayed outdoors.']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.8046875\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.8359375\n",
      "[['The side of a bus parked on the side of a street.', 'The side of a bus parked .', 'The side of a bus parked on the side .'], ['A man in a wet suit stands on a surfboard and rows with a paddle.', 'A man  stands on a surfboard and rows with a paddle.', 'A man in a wet suit stands  and rows with a paddle.', 'A man in a wet suit stands on a surfboard and rows .'], ['A kitchen has wooden cabinets and black countertops.']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7265625\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.78125\n",
      "[['a glass fish bowl with rocks and weeds in it.', 'a glass fish bowl  in it.', 'a glass fish bowl with rocks and weeds .'], ['a glass vase with three pink flowers and a drink'], ['Two televisions that are stacked on top of each other. ', 'Two televisions that are stacked . ', 'Two televisions that are stacked on top . ']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.75\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.7578125\n",
      "[['there is a goat that has a leash on ', 'there is a goat that has a leash  '], ['A young boy taking a swing at a tennis ball', 'A young boy taking a swing '], ['A giraffe is peeking around the side of a wall at the camera.', 'A giraffe is peeking around the side .', 'A giraffe is peeking around the side of a wall .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7578125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.8203125\n",
      "[['a brown bear is walking away from a river', 'a brown bear is walking away '], ['A group of people sitting down to eat and having conversations.'], ['A clock sits above green bushes under a blue sky.', 'A clock sits  under a blue sky.', 'A clock sits above green bushes .']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7578125\n",
      "\n",
      "\n",
      "Clean accuracy normal:  0.7421875\n",
      "[['SIX PEOPLE ON A BUS, WITH FIVE OF THEM ON THEIR ELECTRONICS', 'SIX PEOPLE , WITH FIVE OF THEM ON THEIR ELECTRONICS', 'SIX PEOPLE ON A BUS,  ON THEIR ELECTRONICS', 'SIX PEOPLE ON A BUS, WITH FIVE  ON THEIR ELECTRONICS', 'SIX PEOPLE ON A BUS, WITH FIVE OF THEM '], ['A young boy wearing a baseball uniform holding a baseball bat.'], ['A yellow food truck parked close to a car', 'A yellow food truck parked close ']]\n",
      "torch.Size([128, 128])\n",
      "Clean accuracy ensemble:  0.7734375\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47551/1988151601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormal_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble_clip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ensemble'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_47551/2361653851.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(models, names)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Clean accuracy {name}: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/sriramb/anaconda3/envs/robustlsm/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47551/204949924.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, imgs, texts)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#         processed_imgs = torch.stack([normalize(img).to(device) for img in imgs])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/sriramb/anaconda3/envs/robustlsm/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/sriramb/anaconda3/envs/robustlsm/lib/python3.7/site-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# normalized features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/sriramb/anaconda3/envs/robustlsm/lib/python3.7/site-packages/clip/model.py\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;31m# x.shape = [batch_size, n_ctx, transformer.width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# take features from the eot embedding (eot_token is the highest number in each sequence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test([normal_clip, ensemble_clip],  ['normal', 'ensemble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef805f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text detection\n",
    "def style_text_detect(text, style):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f18de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text transformations\n",
    "\n",
    "def style_text_transform(text, style):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072060c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image detection\n",
    "def style_image_detect(text, style):\n",
    "    pass\n",
    "\n",
    "def clarity_image_detect(text, style):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d453d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transformations\n",
    "def style_image_transform(image, style):\n",
    "    pass\n",
    "\n",
    "def blur_image_transform(image, blur):\n",
    "    return cv2.blur(img,(blur,blur))\n",
    "\n",
    "def object_resize_image_transform(image, obj):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fog_mask = create_fog([ 100 , 100], [20, 100], np.zeros((224, 224)))\n",
    "# fog_mask = create_fog([ 150 , 150], [40, 90], fog_mask)\n",
    "# fog_mask = create_fog([ 200 , 200], [30, 80], fog_mask)\n",
    "# fog_mask = create_fog([ 200 , 100], [20, 70], fog_mask)\n",
    "# fog_mask = create_fog([ 25 , 150], [40, 90], fog_mask)\n",
    "# fog_mask = create_fog([ 50 , 50], [20, 20], fog_mask)\n",
    "\n",
    "# fog_mask = np.expand_dims(fog_mask, axis=-1).astype(np.float32)\n",
    "# fog_mask = fog_mask/fog_mask.max()\n",
    "# orig_img = (imgs[0].permute((1,2,0)).numpy()).astype(np.float32)#\n",
    "\n",
    "# # orig_img[orig_img > 1] = 1\n",
    "# print(orig_img.max(), orig_img.shape, orig_img.dtype)\n",
    "# hsv = cv2.cvtColor(orig_img, cv2.COLOR_RGB2HSV)\n",
    "# hsv[:,:,1:2] = hsv[:,:,1:2]*(1 - fog_mask)\n",
    "# hsv[:,:,2:3] = hsv[:,:,2:3]*(1 - fog_mask) + fog_mask\n",
    "# hsv[:,:,2] *= 0.6\n",
    "# rgb_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "# plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "352c5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fog(mean, std, arr):\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            arr[i][j] += np.exp(-((i-mean[0])**2/(2*std[0]**2) + (j-mean[1])**2/(2*std[1]**2)))\n",
    "    return arr\n",
    "\n",
    "fog_mask = create_fog([ 100 , 100], [20, 100], np.zeros((224, 224)))\n",
    "fog_mask = create_fog([ 150 , 150], [40, 90], fog_mask)\n",
    "fog_mask = create_fog([ 200 , 200], [30, 80], fog_mask)\n",
    "fog_mask = create_fog([ 200 , 100], [20, 70], fog_mask)\n",
    "fog_mask = create_fog([ 25 , 150], [40, 90], fog_mask)\n",
    "fog_mask = create_fog([ 50 , 50], [20, 20], fog_mask)\n",
    "\n",
    "fog_mask = np.expand_dims(fog_mask, axis=-1).astype(np.float32)\n",
    "fog_mask = fog_mask/fog_mask.max()\n",
    "\n",
    "\n",
    "def add_fog(orig_img):\n",
    "    #     orig_img = (imgs[0].permute((1,2,0)).numpy()).astype(np.float32)#\n",
    "\n",
    "    # orig_img[orig_img > 1] = 1\n",
    "    hsv = cv2.cvtColor(orig_img, cv2.COLOR_RGB2HSV)\n",
    "    hsv[:,:,1:2] = hsv[:,:,1:2]*(1 - fog_mask)\n",
    "    hsv[:,:,2:3] = hsv[:,:,2:3]*(1 - fog_mask) + fog_mask\n",
    "    hsv[:,:,2] *= 0.6\n",
    "    rgb_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    return rgb_img\n",
    "\n",
    "err_rain_slant=\"Numeric value between -20 and 20 is allowed\"\n",
    "err_rain_width=\"Width value between 1 and 5 is allowed\"\n",
    "err_rain_length=\"Length value between 0 and 100 is allowed\"\n",
    "\n",
    "def generate_random_lines(imshape,slant,drop_length,rain_type):\n",
    "    drops=[]\n",
    "    area=imshape[0]*imshape[1]\n",
    "    no_of_drops=area//600\n",
    "\n",
    "    if rain_type.lower()=='drizzle':\n",
    "        no_of_drops=area//770\n",
    "        drop_length=10\n",
    "    elif rain_type.lower()=='heavy':\n",
    "        drop_length=30\n",
    "    elif rain_type.lower()=='torrential':\n",
    "        no_of_drops=area//500\n",
    "        drop_length=60\n",
    "\n",
    "    for i in range(no_of_drops): ## If You want heavy rain, try increasing this\n",
    "        if slant<0:\n",
    "            x= np.random.randint(slant,imshape[1])\n",
    "        else:\n",
    "            x= np.random.randint(0,imshape[1]-slant)\n",
    "        y= np.random.randint(0,imshape[0]-drop_length)\n",
    "        drops.append((x,y))\n",
    "    return drops,drop_length\n",
    "\n",
    "def rain_process(image,slant,drop_length,drop_color,drop_width,rain_drops):\n",
    "    imshape = image.shape  \n",
    "    image_t= image.copy()\n",
    "    for rain_drop in rain_drops:\n",
    "        cv2.line(image_t,(rain_drop[0],rain_drop[1]),(rain_drop[0]+slant,rain_drop[1]+drop_length),(0.8, 0.8, 0.8),drop_width)\n",
    "    image= cv2.blur(image_t,(3,3)) ## rainy view are blurry\n",
    "    brightness_coefficient = 0.8 ## rainy days are usually shady \n",
    "    image_HSV = cv2.cvtColor(image, cv2.COLOR_RGB2HLS) ## Conversion to HLS\n",
    "    image_HSV[:,:,1] = image_HSV[:,:,1]*0.8 ## scale pixel values down for channel 1(Lightness)\n",
    "    image_RGB= cv2.cvtColor(image_HSV, cv2.COLOR_HLS2RGB) ## Conversion to RGB\n",
    "    return image_RGB\n",
    "\n",
    "##rain_type='drizzle','heavy','torrential'\n",
    "def add_rain(image,slant=20,drop_length=20,drop_width=1,drop_color=(200,200,200),rain_type='heavy'): ## (200,200,200) a shade of gray\n",
    "    slant_extreme=slant\n",
    "    imshape = image.shape\n",
    "    if slant_extreme==-1:\n",
    "        slant= np.random.randint(-10,10) ##generate random slant if no slant value is given\n",
    "    rain_drops,drop_length= generate_random_lines(imshape,slant,drop_length,rain_type)\n",
    "    image_RGB = rain_process(image,slant_extreme,drop_length,drop_color,drop_width,rain_drops)\n",
    "    return image_RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81f0e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_detect(img, text):\n",
    "    tokens = word_tokenize(text)\n",
    "    vec = [0,0,0]\n",
    "#     if 'rain' in tokens or 'rainy' in tokens:\n",
    "#         vec[0] = 1\n",
    "#     if 'snow' in tokens or 'snowy' in tokens:\n",
    "#         vec[1] = 1\n",
    "#     if 'rain' in tokens or 'rainy' in tokens:\n",
    "#         vec[2] = 1\n",
    "    noun_dict = {'rain': 0, 'snow':1, 'fog':2}\n",
    "    adj_dict = {'rainy': 0, 'snowy':1, 'foggy':2}\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in noun_dict.keys():\n",
    "            vec[noun_dict[tokens[i]]] = 1\n",
    "            tokens[i] = '[NOUN]'\n",
    "        if tokens[i] in adj_dict.keys():\n",
    "            vec[adj_dict[tokens[i]]] = 1\n",
    "            tokens[i] = '[ADJ]'\n",
    "    return vec, ' '.join(tokens)\n",
    "\n",
    "def weather_detect_both(img, text):\n",
    "    return sum(weather_detect(img, text)[0]) == 0\n",
    "\n",
    "import time\n",
    "def weather_transform_both(img, text):\n",
    "    numpy_image = img.permute(1,2,0).numpy().astype(np.float32)\n",
    "    a = time.time()\n",
    "    image_rainy = torch.Tensor(add_rain(numpy_image)).permute(2,0,1)\n",
    "    a = time.time()\n",
    "    image_foggy = torch.Tensor(add_fog(numpy_image)).permute(2,0,1)\n",
    "    image_rainy = image_rainy.clamp(0,1)\n",
    "    image_foggy = image_foggy.clamp(0,1)\n",
    "#     plt.imsave('./examples/transformed_rainy_'+text+'.png', image_rainy.permute(1,2,0).numpy())\n",
    "#     plt.imsave('./examples/transformed_snowy_'+text+'.png', image_snowy.permute(1,2,0).numpy())\n",
    "#     plt.imsave('./examples/transformed_foggy_'+text+'.png', image_foggy.permute(1,2,0).numpy())\n",
    "    tr_imgs = torch.stack([image_rainy, image_foggy])\n",
    "    tr_text = [\"a photo of \" + text[:1].lower() + text[1:].strip('.') + ' in the ' + w for w in ['rain', 'fog']]\n",
    "    return tr_text, tr_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c874f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = (imgs[1].permute((1,2,0)).numpy()*255).astype(int)\n",
    "# img = cv2.imread(\"examples/transformed_foggy_A little girl holding wet broccoli in her hand. .png\")\n",
    "# image_HLS = cv2.cvtColor(img.astype(np.float32)/255,cv2.COLOR_RGB2HLS) ## Conversion to HLS\n",
    "# image_HLS = np.array(image_HLS, dtype = np.float64) \n",
    "#     brightness_coefficient = 1.2\n",
    "#     imshape = image.shape\n",
    "#     snow_point=snow_coeff ## increase this for more snow\n",
    "#     image_HLS[:,:,1][image_HLS[:,:,1]<snow_point] = image_HLS[:,:,1][image_HLS[:,:,1]<snow_point]*brightness_coefficient ## scale pixel values up for channel 1(Lightness)\n",
    "#     image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255\n",
    "# image_HLS = np.array(image_HLS, dtype = np.uint8)\n",
    "# image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b789b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_both(imgs, texts , transform_fn, detect_fn):\n",
    "    rel_imgs = []\n",
    "    rel_texts = []\n",
    "    for img, text in zip(imgs, texts):\n",
    "        if detect_fn(img, text):\n",
    "            tr_texts, tr_imgs = transform_fn(img, text)\n",
    "            rel_imgs.append(tr_imgs)\n",
    "            rel_texts.append(tr_texts)\n",
    "    rel_imgs = torch.stack(rel_imgs)\n",
    "#     rel_texts = flatten(rel_texts)\n",
    "    return rel_imgs, rel_texts\n",
    "\n",
    "\n",
    "@indent\n",
    "def test_on_transformed_both(model, tr_imgs, tr_texts):\n",
    "    dim_size = len(tr_imgs[0])\n",
    "    flat_texts = flatten(tr_texts)\n",
    "    flat_imgs = tr_imgs.reshape(-1, *tr_imgs.shape[2:])\n",
    "    sims = sim(model, flat_imgs, flat_texts)\n",
    "    preds = sims.argmax(-1)\n",
    "    gt = torch.arange(len(flat_imgs))\n",
    "    gen_preds = torch.div(preds, dim_size, rounding_mode='floor')\n",
    "    gen_gt = torch.div(gt, dim_size, rounding_mode='floor')\n",
    "    gen_preds_2 = preds % dim_size\n",
    "    gen_gt_2 = gt % dim_size\n",
    "    print(\"Reduced probability (across samples): \", float((gen_preds == gen_gt).float().mean()))\n",
    "    print(\"Reduced probability (across pert dimension): \", float((gen_preds_2 == gen_gt_2).float().mean()))\n",
    "    print(\"Average dimension specific probability: \", float((preds == gt).float().mean()))\n",
    "#     preds = preds.reshape(-1, dim_size)\n",
    "#     gt = gt.reshape(-1, dim_size)\n",
    "#     print(\"Probability for dim 0: \", float((preds[:,0] == gt[:,0]).float().mean()))\n",
    "#     print(\"Probability for dim 1: \", float((preds[:,1] == gt[:,1]).float().mean()))\n",
    "    return preds, gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3049bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(10)%2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0b14e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean accuracy:  0.765625\n",
      "Combined acc:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test_on_transformed_both() missing 1 required positional argument: 'tr_texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47551/2374809338.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdim_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a photo of rain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a photo of fog\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Combined acc:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_on_transformed_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample text probes acc:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpreds_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_on_transformed_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47551/2830761448.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: test_on_transformed_both() missing 1 required positional argument: 'tr_texts'"
     ]
    }
   ],
   "source": [
    "all_tr_imgs, all_tr_texts = [], []\n",
    "detect_fn = lambda a, b : True\n",
    "b = 0\n",
    "for imgs, texts in test_loader:\n",
    "    imgs = torch.stack(imgs)\n",
    "    texts = [t[0] for t in texts]\n",
    "#     print(imgs.shape, len(texts))\n",
    "    a = sim(model, imgs, [\"a photo of \"+t for t in texts])\n",
    "    print(\"Clean accuracy: \",float((a.argmax(-1) == torch.arange(batch_size)).float().mean()))\n",
    "    tr_imgs, tr_texts = transform_both(imgs, texts, weather_transform_both, detect_fn)\n",
    "    sample_texts = [[\"a photo of \"+t for _ in range(2)] for t in texts]\n",
    "    dim_texts = [[\"a photo of rain\", \"a photo of fog\"] for _ in range(len(texts))]\n",
    "    print(\"Combined acc:\")\n",
    "    preds, gt = test_on_transformed_both(model, tr_imgs, tr_texts)\n",
    "    print(\"Sample text probes acc:\")\n",
    "    preds_1, gt_1 = test_on_transformed_both(model, tr_imgs, sample_texts)\n",
    "    print(\"Dimension text probes acc\")\n",
    "    preds_2, gt_2 = test_on_transformed_both(model, tr_imgs, dim_texts)\n",
    "    \n",
    "    dim_size = len(tr_imgs[0])\n",
    "    gen_preds_1 = torch.div(preds_1, dim_size, rounding_mode='floor')\n",
    "    gen_gt_1 = torch.div(gt_1, dim_size, rounding_mode='floor')\n",
    "    gen_preds_2 = preds_2 % dim_size\n",
    "    gen_gt_2 = gt_2 % dim_size\n",
    "    print(\"Individual text probes acc multiplied:\", float(np.logical_and(gen_preds_1 == gen_gt_1 , gen_preds_2 == gen_gt_2).float().mean()))\n",
    "\n",
    "    all_tr_imgs.append(tr_imgs)\n",
    "    all_tr_texts.append(tr_texts)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6cbd686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog'],\n",
       " ['a photo of rain', 'a photo of fog']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb527dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A cake shaped like a horse with white frosting and decorative candies in different colors.',\n",
       " 'Racer riding a dirt bike on a race course.',\n",
       " 'a street sign on a pole on a city street',\n",
       " 'A baseball glove and ball in a grassy field.',\n",
       " 'An office with a corner desk full of three desk top computers and paper.',\n",
       " 'A blue double decker bus driving in front of a green double decker.',\n",
       " 'Trays of various donuts in a glass case.',\n",
       " 'a person riding skis on a snowy surface',\n",
       " 'A sandwich and chips are on a plate with a cup of coffee, cell phone, and a newspaper nearby.',\n",
       " 'A beach filled with docked boats next to the ocean.',\n",
       " 'A decorated VW van is used as a display on a wall.',\n",
       " 'A woman holds a tennis racket and tennis ball.',\n",
       " 'A parade going on in the street while people on the side watch.',\n",
       " 'A blue bus in multiple lanes on a city street.',\n",
       " 'This is one man with two different sides to him.',\n",
       " 'A bright yellow fire hydrant on the sidewalk by a road.',\n",
       " 'A graphic of a girl holding skates on her shoulders.',\n",
       " 'A bed covered in a plaid blanket sitting under a mirror.',\n",
       " 'A fork and a piece of food sit on a plate. ',\n",
       " 'A room with two tables sitting around a fire place.',\n",
       " 'A table topped with four white bowls of food.',\n",
       " 'a person sitting on a city street talking on a cell phone',\n",
       " 'A group of people standing around a cake.',\n",
       " 'three people are on a white surf board water and sail boats',\n",
       " 'A man riding a skateboard on at a public skating park.',\n",
       " 'women on a tennis court playing a game of tennis ',\n",
       " 'A person surfing on their surfboard in the ocean.',\n",
       " 'A person on a skateboard does an air trick.',\n",
       " 'the bike has a carriage in the back',\n",
       " 'A woman walking past a yellow fire hydrant with a chili pepper on it.',\n",
       " 'the firetruck is parked next to a police car',\n",
       " 'A green sign sitting next to a bunch of leafy trees.',\n",
       " 'A man sitting on a bench right on a bay of water.',\n",
       " 'Elephants getting bathed by a man in blue.',\n",
       " 'A girl on a bed sits near the corner of the room.',\n",
       " 'A giant sandwich sitting on top of a white plate.',\n",
       " 'A sealed mason which contains layers of different kinds of vegetables.',\n",
       " 'A group of elephants walking along a grassy field.',\n",
       " 'This meal has a chicken pattie with a bottle of pepper near it.',\n",
       " 'this lady is walking along the shore on a beach',\n",
       " 'A small twin bed sitting underneath a poster on a wall.',\n",
       " 'Two cars on a road below various traffic lights.',\n",
       " 'Three young men play soccer in a gymnasium',\n",
       " 'A red duble decker bus with brick buildings in the background.',\n",
       " 'a kid is running wwith a red bat',\n",
       " 'A young man riding up the side of a dry swimming pol on a skateboard.',\n",
       " 'A person on some skis jumping in the air.',\n",
       " 'A young brown cow is walking down the dirt road.',\n",
       " 'A very long bright yellow train on some tracks.',\n",
       " 'A bike is tied to a pole while a sufer walks on the sand to the water.',\n",
       " 'A close up of a banana next to a cup with liquid.',\n",
       " 'A yellow and white train traveling down train tracks.',\n",
       " 'A young child riding on top of a skateboard.',\n",
       " 'A blue umbrella over a tall wooden fence.',\n",
       " 'A living room with chairs, window, and table in it.',\n",
       " 'there is a yellow truck sign next to a fire hydrant',\n",
       " 'Luggage carts bringing luggage to an airplane on the tarmac.',\n",
       " 'A couple of large white airplanes parked in a stationary position.',\n",
       " 'This is a giraffe standing in an enclosure.',\n",
       " 'A leather case filled with metal pairs scissors.',\n",
       " 'A kitchen sink sitting under a kitchen window.',\n",
       " 'A group of tables and chairs covered by umbrellas.',\n",
       " 'A close up view of a broccoli and corn sitting on a plate.',\n",
       " 'A group of birds sitting on branches that are above water in a storm drainage ditch.',\n",
       " \"A dog running across a field with a frisbee in it's mouth.\",\n",
       " 'Two plates of food with different kinds of food on them.',\n",
       " 'A white and red thermometer stuck in an orange.',\n",
       " 'A small bathroom toilet betwwen a heater and a sink',\n",
       " 'A few bikes parked in front of a blue building.',\n",
       " 'A city street sign on the corner of a street.',\n",
       " 'A container of food and a fork on a table.',\n",
       " 'a young man standing near a river watching a boat of people pass by',\n",
       " 'Plate of hotdogs smothered in condiments with tater tots',\n",
       " 'Eggs, orange and vegetables are sitting on a plate.',\n",
       " 'A cat climbing into a bathroom sink looking at someone',\n",
       " 'A well decorated bakers shelf with cups, and a vase of flowers.',\n",
       " 'A snowboarder takes a curve on a snowy slope.',\n",
       " 'A man standing in front of refrigerators stacked on top of each other.',\n",
       " 'Several types of donuts including cake, glazed and an apple fritter.',\n",
       " 'A group of people sitting around a table with a pizza on it.',\n",
       " 'The desk is cluttered with CDs, electronics and toys.',\n",
       " 'A plate and a bowl of food sitting on a tray near two cups on a table.',\n",
       " 'A couple of zebra standing next to each other.',\n",
       " 'A man smiles and talks in a microphone.',\n",
       " 'A group of young men standing on a basketball court.',\n",
       " 'A woman wearing a patterned backpack walks by the luggage pick up of an airport.',\n",
       " 'A view of cars and trucks parked along side a street. ',\n",
       " 'Two bowls filled with broccoli soup on top of a table.',\n",
       " 'A man bending over to look inside of a fridge.',\n",
       " 'A plate of red stuffed peppers with a side of vegetables and carrots.',\n",
       " 'A young boy gets ready to hit a baseball.',\n",
       " 'The kitchen sink is clean and ready to use. ',\n",
       " 'Two woman are showing off a pickup truck.',\n",
       " 'A plate that has sausage, toast, and vegetables on it.',\n",
       " 'A white toilet tin a bathroom sitting next to a sink.',\n",
       " 'A toilet and a urinal separated by a gray partition',\n",
       " 'A person takes the first slice of a pizza pie.',\n",
       " 'A couple of men standing on a baseball field in uniform.',\n",
       " 'a plate with some fruits and a bread',\n",
       " 'A woman sitting at a table eating a plate of food.',\n",
       " 'A picture of a person sitting on a bench.',\n",
       " 'There are people getting off of the commuter train',\n",
       " 'Two people sitting on the couch and playing a game.',\n",
       " 'A giraffe standing on top of a grass covered field.',\n",
       " 'A man in brown jacket doing a trick on a snowboard.',\n",
       " 'A group of people that are standing in a living room.',\n",
       " 'An orange and white cat laying on top of a bed.',\n",
       " 'A kitchen filled with white cabinets and a wooden ladder.',\n",
       " 'a little boy jumping to put the basketball in the hoop',\n",
       " 'A stove and a sink in a room.',\n",
       " 'A man jumping catching a frisbee between his legs.',\n",
       " 'The head and arm of an unpainted lego man.',\n",
       " 'Police man wearing a white shirt and black pants on a motorcycle',\n",
       " 'A black and white picture of a group of people.',\n",
       " 'A group of people standing together on a field smiling.',\n",
       " 'A group of soldiers saluting an airforce cargo plane.',\n",
       " 'A large gray elephant walking through a marsh.',\n",
       " 'Young girl dressed in blue and pink skiing down a hill.',\n",
       " 'a man signing a tie on a table',\n",
       " 'A plate with food sitting on a table.',\n",
       " 'Small bird hanging off the side of a cage bird feeder',\n",
       " 'a close up of a dog laying on its back',\n",
       " 'A woman cutting up vegetables on top of a wooden cutting board.',\n",
       " 'Two men looking at a boat full of people next to a Battleship.',\n",
       " 'a large area of water and a bridge',\n",
       " 'A woman sitting on a bench reading a paper.',\n",
       " 'A woman waits to receive the ball back from her opponent.',\n",
       " 'A group of what appears to be young people playing on a game system.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b376eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a photo of A man working on his laptop while a woman reads something on his desk and another eats in the background in the rain',\n",
       "  'a photo of A man working on his laptop while a woman reads something on his desk and another eats in the background in the fog'],\n",
       " ['a photo of two giraffes in a wooded area looking straight at the camera in the rain',\n",
       "  'a photo of two giraffes in a wooded area looking straight at the camera in the fog'],\n",
       " ['a photo of A black cat is sitting in a round blue chair in the rain',\n",
       "  'a photo of A black cat is sitting in a round blue chair in the fog'],\n",
       " ['a photo of A woman laying on top of a bed in red shoes in the rain',\n",
       "  'a photo of A woman laying on top of a bed in red shoes in the fog'],\n",
       " ['a photo of A burned up old outdated computer sitting on a desk in the rain',\n",
       "  'a photo of A burned up old outdated computer sitting on a desk in the fog'],\n",
       " ['a photo of Large group of people all sitting lined up across a large table in the rain',\n",
       "  'a photo of Large group of people all sitting lined up across a large table in the fog'],\n",
       " ['a photo of This picture appears to be a model double decker bus in the rain',\n",
       "  'a photo of This picture appears to be a model double decker bus in the fog'],\n",
       " ['a photo of a man sitting on a couch holding a smart phone next to a laptop in the rain',\n",
       "  'a photo of a man sitting on a couch holding a smart phone next to a laptop in the fog'],\n",
       " ['a photo of A neat yellow bed in a room with white and blue walls in the rain',\n",
       "  'a photo of A neat yellow bed in a room with white and blue walls in the fog'],\n",
       " ['a photo of A very cute cat laying in a small sink in the rain',\n",
       "  'a photo of A very cute cat laying in a small sink in the fog'],\n",
       " ['a photo of A metro bus, driving up a hull on a small street near many houses in the rain',\n",
       "  'a photo of A metro bus, driving up a hull on a small street near many houses in the fog'],\n",
       " ['a photo of there are many sheep together, one can be seen smiling in the rain',\n",
       "  'a photo of there are many sheep together, one can be seen smiling in the fog'],\n",
       " ['a photo of Four pairs of scissors sitting next to each other under a picture of a demon in the rain',\n",
       "  'a photo of Four pairs of scissors sitting next to each other under a picture of a demon in the fog'],\n",
       " ['a photo of A large polar bear licking the head of one of two small polar bears in the rain',\n",
       "  'a photo of A large polar bear licking the head of one of two small polar bears in the fog'],\n",
       " ['a photo of An open faced barbecue sandwich is on a tray in the rain',\n",
       "  'a photo of An open faced barbecue sandwich is on a tray in the fog'],\n",
       " ['a photo of A bus that is parked inside of a building in the rain',\n",
       "  'a photo of A bus that is parked inside of a building in the fog'],\n",
       " ['a photo of A German shepherd laying on the ground with a frisbee between its paws  in the rain',\n",
       "  'a photo of A German shepherd laying on the ground with a frisbee between its paws  in the fog'],\n",
       " ['a photo of a bowl of fruit with other fruit near by in the rain',\n",
       "  'a photo of a bowl of fruit with other fruit near by in the fog'],\n",
       " ['a photo of A man riding a skateboard up the side of a ramp in the rain',\n",
       "  'a photo of A man riding a skateboard up the side of a ramp in the fog'],\n",
       " ['a photo of A group of young women standing in front of young men at a party in the rain',\n",
       "  'a photo of A group of young women standing in front of young men at a party in the fog'],\n",
       " ['a photo of a person in some water being pulled by an air chute  in the rain',\n",
       "  'a photo of a person in some water being pulled by an air chute  in the fog'],\n",
       " ['a photo of A woman kneeling down by a green bench near a black cat in the rain',\n",
       "  'a photo of A woman kneeling down by a green bench near a black cat in the fog'],\n",
       " ['a photo of A person in a costume on the back of a car in the rain',\n",
       "  'a photo of A person in a costume on the back of a car in the fog'],\n",
       " ['a photo of People that are making a pizza from start to finish.  in the rain',\n",
       "  'a photo of People that are making a pizza from start to finish.  in the fog'],\n",
       " ['a photo of A group of old fashioned trucks parked side by side in the rain',\n",
       "  'a photo of A group of old fashioned trucks parked side by side in the fog'],\n",
       " ['a photo of A lone zebra is eating grass in a field in the rain',\n",
       "  'a photo of A lone zebra is eating grass in a field in the fog'],\n",
       " ['a photo of A street in front of a tall building filled with traffic in the rain',\n",
       "  'a photo of A street in front of a tall building filled with traffic in the fog'],\n",
       " ['a photo of A man drinking wine at a wine tasting in the rain',\n",
       "  'a photo of A man drinking wine at a wine tasting in the fog'],\n",
       " ['a photo of An open field with sand, grass, and trash cans in the rain',\n",
       "  'a photo of An open field with sand, grass, and trash cans in the fog'],\n",
       " ['a photo of Man on surf board surfing on ocean waves in the rain',\n",
       "  'a photo of Man on surf board surfing on ocean waves in the fog'],\n",
       " ['a photo of An older woman sitting on a chair looking at a phone in the rain',\n",
       "  'a photo of An older woman sitting on a chair looking at a phone in the fog'],\n",
       " ['a photo of A bathroom with  his and her sinks under a large mirror in the rain',\n",
       "  'a photo of A bathroom with  his and her sinks under a large mirror in the fog'],\n",
       " ['a photo of The young woman is kicking a soccer ball into the air.  in the rain',\n",
       "  'a photo of The young woman is kicking a soccer ball into the air.  in the fog'],\n",
       " ['a photo of many traffic lights over a city road with a dark sky in the rain',\n",
       "  'a photo of many traffic lights over a city road with a dark sky in the fog'],\n",
       " ['a photo of A pony with a saddle on its back crossing a bridge.  in the rain',\n",
       "  'a photo of A pony with a saddle on its back crossing a bridge.  in the fog'],\n",
       " ['a photo of A person holding a dog for another dog to smell in the rain',\n",
       "  'a photo of A person holding a dog for another dog to smell in the fog'],\n",
       " ['a photo of A table set out with a plate of sandwiches and a cup of coffee.  in the rain',\n",
       "  'a photo of A table set out with a plate of sandwiches and a cup of coffee.  in the fog'],\n",
       " ['a photo of Three men and a woman sit at a high table with wine glasses on it in the rain',\n",
       "  'a photo of Three men and a woman sit at a high table with wine glasses on it in the fog'],\n",
       " ['a photo of This photograph of a surfer entering the ocean is breathtaking in the rain',\n",
       "  'a photo of This photograph of a surfer entering the ocean is breathtaking in the fog'],\n",
       " ['a photo of Six apples and six oranges lined up in front of window in the rain',\n",
       "  'a photo of Six apples and six oranges lined up in front of window in the fog'],\n",
       " ['a photo of A flat screen TV sitting on top of a step in a bathroom in the rain',\n",
       "  'a photo of A flat screen TV sitting on top of a step in a bathroom in the fog'],\n",
       " ['a photo of Two people going through belongings from the truck of a car in the rain',\n",
       "  'a photo of Two people going through belongings from the truck of a car in the fog'],\n",
       " ['a photo of There is an airplane on a airport runway in the rain',\n",
       "  'a photo of There is an airplane on a airport runway in the fog'],\n",
       " ['a photo of A bunch of jets that are flying in the sky in the rain',\n",
       "  'a photo of A bunch of jets that are flying in the sky in the fog'],\n",
       " ['a photo of A group of  young boys riding skis on a snow covered ski slope in the rain',\n",
       "  'a photo of A group of  young boys riding skis on a snow covered ski slope in the fog'],\n",
       " ['a photo of Lady wearing a black tie with green and lavender eye shadow in the rain',\n",
       "  'a photo of Lady wearing a black tie with green and lavender eye shadow in the fog'],\n",
       " ['a photo of Several bunches of bananas are displayed at a market in the rain',\n",
       "  'a photo of Several bunches of bananas are displayed at a market in the fog'],\n",
       " ['a photo of The motorcycle is sitting on the side of the street in the rain',\n",
       "  'a photo of The motorcycle is sitting on the side of the street in the fog'],\n",
       " ['a photo of A huge crowd of people standing on a corner of a very busy intersection.  in the rain',\n",
       "  'a photo of A huge crowd of people standing on a corner of a very busy intersection.  in the fog'],\n",
       " ['a photo of A white fire hydrant on a sidewalk next to a building in the rain',\n",
       "  'a photo of A white fire hydrant on a sidewalk next to a building in the fog'],\n",
       " ['a photo of A vase with flowers is on a table in the rain',\n",
       "  'a photo of A vase with flowers is on a table in the fog'],\n",
       " ['a photo of A kitchen with marble counter tops and black appliances in the rain',\n",
       "  'a photo of A kitchen with marble counter tops and black appliances in the fog'],\n",
       " ['a photo of A black bear sitting on a large stone next to a pond in the rain',\n",
       "  'a photo of A black bear sitting on a large stone next to a pond in the fog'],\n",
       " ['a photo of A cow drinking milk from baby bottle with an orange top in the rain',\n",
       "  'a photo of A cow drinking milk from baby bottle with an orange top in the fog'],\n",
       " ['a photo of There is a dog sitting on a park bench in the rain',\n",
       "  'a photo of There is a dog sitting on a park bench in the fog'],\n",
       " ['a photo of A group of young people eating pizza at a table in the rain',\n",
       "  'a photo of A group of young people eating pizza at a table in the fog'],\n",
       " ['a photo of An elephant in the wild walking by a house in the rain',\n",
       "  'a photo of An elephant in the wild walking by a house in the fog'],\n",
       " ['a photo of A group of people trying out the new Nintendo Wii U in the rain',\n",
       "  'a photo of A group of people trying out the new Nintendo Wii U in the fog'],\n",
       " ['a photo of A bus parked in front of a house in the rain',\n",
       "  'a photo of A bus parked in front of a house in the fog'],\n",
       " ['a photo of a few cars that has kites flying above in the rain',\n",
       "  'a photo of a few cars that has kites flying above in the fog'],\n",
       " ['a photo of A clean kitchen area with a yellow dining table in the rain',\n",
       "  'a photo of A clean kitchen area with a yellow dining table in the fog'],\n",
       " ['a photo of A young woman standing on a tennis court holding a racquet in the rain',\n",
       "  'a photo of A young woman standing on a tennis court holding a racquet in the fog'],\n",
       " ['a photo of A group of giraffes standing in the grass near trees in the rain',\n",
       "  'a photo of A group of giraffes standing in the grass near trees in the fog'],\n",
       " ['a photo of An old blue truck is parked near a tree with branches.  in the rain',\n",
       "  'a photo of An old blue truck is parked near a tree with branches.  in the fog'],\n",
       " ['a photo of A grey black and white cat laying in a chair in the rain',\n",
       "  'a photo of A grey black and white cat laying in a chair in the fog'],\n",
       " ['a photo of some people in the snow flying kites and some trees in the rain',\n",
       "  'a photo of some people in the snow flying kites and some trees in the fog'],\n",
       " ['a photo of A woman standing next to two children flying a kite in the rain',\n",
       "  'a photo of A woman standing next to two children flying a kite in the fog'],\n",
       " ['a photo of A man looking out the window while holding food from a full plate.  in the rain',\n",
       "  'a photo of A man looking out the window while holding food from a full plate.  in the fog'],\n",
       " ['a photo of The view of a old bathroom with no shower curtain in the rain',\n",
       "  'a photo of The view of a old bathroom with no shower curtain in the fog'],\n",
       " ['a photo of A mini train about to go through a tunnel in the rain',\n",
       "  'a photo of A mini train about to go through a tunnel in the fog'],\n",
       " ['a photo of A nest filled with baby birds crying for their mother in the rain',\n",
       "  'a photo of A nest filled with baby birds crying for their mother in the fog'],\n",
       " ['a photo of Women holding a toddler looking at a horse in the rain',\n",
       "  'a photo of Women holding a toddler looking at a horse in the fog'],\n",
       " ['a photo of A white plate filled with a variety of foods  in the rain',\n",
       "  'a photo of A white plate filled with a variety of foods  in the fog'],\n",
       " ['a photo of A tray filled with plates and dishes full of food in the rain',\n",
       "  'a photo of A tray filled with plates and dishes full of food in the fog'],\n",
       " ['a photo of Several zebras and rhinoceros grazing together on a sunny day.  in the rain',\n",
       "  'a photo of Several zebras and rhinoceros grazing together on a sunny day.  in the fog'],\n",
       " ['a photo of A dim living room with modern furniture and potted plants in the rain',\n",
       "  'a photo of A dim living room with modern furniture and potted plants in the fog'],\n",
       " ['a photo of A bathroom showing tub, sink, and toilet  in the rain',\n",
       "  'a photo of A bathroom showing tub, sink, and toilet  in the fog'],\n",
       " ['a photo of Snowman wearing hat and scarf outdoors on winter day in the rain',\n",
       "  'a photo of Snowman wearing hat and scarf outdoors on winter day in the fog'],\n",
       " [\"a photo of A father poses with his small son, who is sitting on dad's motorcycle.  in the rain\",\n",
       "  \"a photo of A father poses with his small son, who is sitting on dad's motorcycle.  in the fog\"],\n",
       " ['a photo of Apples sitting in front of a hookah pipe in the rain',\n",
       "  'a photo of Apples sitting in front of a hookah pipe in the fog'],\n",
       " ['a photo of A man who is performing a trick on a skateboard in the rain',\n",
       "  'a photo of A man who is performing a trick on a skateboard in the fog'],\n",
       " ['a photo of a woman in blue and red looks in the refrigerator in the rain',\n",
       "  'a photo of a woman in blue and red looks in the refrigerator in the fog'],\n",
       " ['a photo of Several train tracks with a train on one of them in the rain',\n",
       "  'a photo of Several train tracks with a train on one of them in the fog'],\n",
       " ['a photo of A boy holding a bat to swing at a ball in the rain',\n",
       "  'a photo of A boy holding a bat to swing at a ball in the fog'],\n",
       " ['a photo of A couple of hoagie sandwiches next to a bowl of soup in the rain',\n",
       "  'a photo of A couple of hoagie sandwiches next to a bowl of soup in the fog'],\n",
       " ['a photo of A line of motorcycles are parked in a row ranging from vintage to rocket in the rain',\n",
       "  'a photo of A line of motorcycles are parked in a row ranging from vintage to rocket in the fog'],\n",
       " ['a photo of A man with a hat on is sitting at a table waiting to eat in the rain',\n",
       "  'a photo of A man with a hat on is sitting at a table waiting to eat in the fog'],\n",
       " ['a photo of A street that has many various businesses written in a foreign language in the rain',\n",
       "  'a photo of A street that has many various businesses written in a foreign language in the fog'],\n",
       " ['a photo of Four man standing up in a room holding Wii controllers in the rain',\n",
       "  'a photo of Four man standing up in a room holding Wii controllers in the fog'],\n",
       " ['a photo of An oriental musical group gathered in a field in the rain',\n",
       "  'a photo of An oriental musical group gathered in a field in the fog'],\n",
       " ['a photo of A baby girl sitting on a blanket holding a teddy bear in the rain',\n",
       "  'a photo of A baby girl sitting on a blanket holding a teddy bear in the fog'],\n",
       " ['a photo of A white bathroom with a large window in it in the rain',\n",
       "  'a photo of A white bathroom with a large window in it in the fog'],\n",
       " ['a photo of A white bath tub sitting next to a toilet in the rain',\n",
       "  'a photo of A white bath tub sitting next to a toilet in the fog'],\n",
       " ['a photo of A man is standing behind a counter of baked goods in the rain',\n",
       "  'a photo of A man is standing behind a counter of baked goods in the fog'],\n",
       " ['a photo of A car stopped at a traffic light waiting for it to change in the rain',\n",
       "  'a photo of A car stopped at a traffic light waiting for it to change in the fog'],\n",
       " ['a photo of A man with a white beard and cigarette in his mouth driving a motorcycle with two seats in the rain',\n",
       "  'a photo of A man with a white beard and cigarette in his mouth driving a motorcycle with two seats in the fog'],\n",
       " [\"a photo of A refrigerator with lots of food with its' doors open in the rain\",\n",
       "  \"a photo of A refrigerator with lots of food with its' doors open in the fog\"],\n",
       " [\"a photo of One of the two elephants in the water is holding grass with it's trunk in the rain\",\n",
       "  \"a photo of One of the two elephants in the water is holding grass with it's trunk in the fog\"],\n",
       " ['a photo of Classic jet fighter on display in front of nice home in the rain',\n",
       "  'a photo of Classic jet fighter on display in front of nice home in the fog'],\n",
       " ['a photo of A person that is on the front of a can in the rain',\n",
       "  'a photo of A person that is on the front of a can in the fog'],\n",
       " ['a photo of Many vehicles are parked on the side of the road in the rain',\n",
       "  'a photo of Many vehicles are parked on the side of the road in the fog'],\n",
       " ['a photo of A display of chocolate donuts and some pink and white cookies in the rain',\n",
       "  'a photo of A display of chocolate donuts and some pink and white cookies in the fog'],\n",
       " ['a photo of A table is laden with treats and decorations for a wedding in the rain',\n",
       "  'a photo of A table is laden with treats and decorations for a wedding in the fog'],\n",
       " ['a photo of a carrot some celery and a knife on a plastic cutting board in the rain',\n",
       "  'a photo of a carrot some celery and a knife on a plastic cutting board in the fog'],\n",
       " ['a photo of A man performing a trick on a skateboard in the rain',\n",
       "  'a photo of A man performing a trick on a skateboard in the fog'],\n",
       " ['a photo of two men playing a video game with controllers  in the rain',\n",
       "  'a photo of two men playing a video game with controllers  in the fog'],\n",
       " ['a photo of Three sheep in the process of running on grass outside in the rain',\n",
       "  'a photo of Three sheep in the process of running on grass outside in the fog'],\n",
       " ['a photo of A person riding a motorcycle down a rural country road in the rain',\n",
       "  'a photo of A person riding a motorcycle down a rural country road in the fog'],\n",
       " ['a photo of A red and gray passenger train traveling on train tracks in the rain',\n",
       "  'a photo of A red and gray passenger train traveling on train tracks in the fog'],\n",
       " ['a photo of Man flying kite at beach on bright sunny day in the rain',\n",
       "  'a photo of Man flying kite at beach on bright sunny day in the fog'],\n",
       " ['a photo of A man and a woman hold each other in this photograph in the rain',\n",
       "  'a photo of A man and a woman hold each other in this photograph in the fog'],\n",
       " ['a photo of a room with a lamp a couch a red rug and some books in the rain',\n",
       "  'a photo of a room with a lamp a couch a red rug and some books in the fog'],\n",
       " ['a photo of A man standing with his back to a mirror, but his reflection facing him.  in the rain',\n",
       "  'a photo of A man standing with his back to a mirror, but his reflection facing him.  in the fog'],\n",
       " ['a photo of Colorful scenery jam packed with surfboards and a tanned surfer in the rain',\n",
       "  'a photo of Colorful scenery jam packed with surfboards and a tanned surfer in the fog'],\n",
       " ['a photo of a man kneeling down as he rides a skateboard beside another person  in the rain',\n",
       "  'a photo of a man kneeling down as he rides a skateboard beside another person  in the fog'],\n",
       " ['a photo of An individual with a yellow apron next to a pile of green bananas.  in the rain',\n",
       "  'a photo of An individual with a yellow apron next to a pile of green bananas.  in the fog'],\n",
       " ['a photo of Young snowboarder being guided on slope by adult in the rain',\n",
       "  'a photo of Young snowboarder being guided on slope by adult in the fog'],\n",
       " ['a photo of A large white clock tower next to an old style building in the rain',\n",
       "  'a photo of A large white clock tower next to an old style building in the fog'],\n",
       " ['a photo of A large red double decker bus traveling through a small town in the rain',\n",
       "  'a photo of A large red double decker bus traveling through a small town in the fog'],\n",
       " ['a photo of A newly married man and a woman walking hand in hand in the rain',\n",
       "  'a photo of A newly married man and a woman walking hand in hand in the fog'],\n",
       " ['a photo of A blue living room with fireplace, potted plant, couch and chairs in the rain',\n",
       "  'a photo of A blue living room with fireplace, potted plant, couch and chairs in the fog'],\n",
       " ['a photo of The tennis player looks upset about the game in the rain',\n",
       "  'a photo of The tennis player looks upset about the game in the fog'],\n",
       " ['a photo of a child at a desk with a laptop in the rain',\n",
       "  'a photo of a child at a desk with a laptop in the fog'],\n",
       " ['a photo of A pair of rusty scissors sitting on top of a white rope and rulers in the rain',\n",
       "  'a photo of A pair of rusty scissors sitting on top of a white rope and rulers in the fog'],\n",
       " ['a photo of a red and white subway train stopped in the station  in the rain',\n",
       "  'a photo of a red and white subway train stopped in the station  in the fog'],\n",
       " ['a photo of A close-up of a zebra looking back behind him in the rain',\n",
       "  'a photo of A close-up of a zebra looking back behind him in the fog'],\n",
       " ['a photo of A man on a skateboard performing a trick in the rain',\n",
       "  'a photo of A man on a skateboard performing a trick in the fog'],\n",
       " ['a photo of A street sign with stick figures on it in the rain',\n",
       "  'a photo of A street sign with stick figures on it in the fog']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21bb1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A        DET   <════════════════╗   det\n",
      "couple   NOUN  ═══════════════╗═╝═╗ ROOT\n",
      "of       ADP   ═════════════╗<╝   ║ prep\n",
      "men      NOUN  ═══════════╗<╝     ║ pobj\n",
      "standing VERB  ═══════╗═╗<╝       ║ acl\n",
      "on       ADP   ═════╗<╝ ║         ║ prep\n",
      "a        DET   <══╗ ║   ║         ║ det\n",
      "baseball NOUN  <╗ ║ ║   ║         ║ compound\n",
      "field    NOUN  ═╝═╝<╝   ║         ║ pobj\n",
      "in       ADP   ═╗<══════╝         ║ prep\n",
      "uniform  NOUN  <╝                 ║ pobj\n",
      ".        PUNCT <══════════════════╝ punct\n"
     ]
    }
   ],
   "source": [
    "deplacy.render(en('A couple of men standing on a baseball field in uniform.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1c08706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093f1017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A      DET  <════╗     det\n",
      "very   ADV  <╗   ║     advmod\n",
      "cute   ADJ  ═╝<╗ ║     amod\n",
      "cat    NOUN ═══╝═╝<══╗ nsubj\n",
      "laying VERB ═══════╗═╝ ROOT\n",
      "in     ADP  ═════╗<╝   prep\n",
      "a      DET  <══╗ ║     det\n",
      "small  ADJ  <╗ ║ ║     amod\n",
      "sink   NOUN ═╝═╝<╝     pobj\n"
     ]
    }
   ],
   "source": [
    "deplacy.render(en('A very cute cat laying in a small sink'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a429673",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "'The young girl is feeding the young boy.'.strip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "from object_remove_inpaint.src.edge_connect import EdgeConnect\n",
    "from object_remove_inpaint.src.config import Config\n",
    "\n",
    "config = Config('../object_remove_inpaint/checkpoints/config.yml')\n",
    "config.MODE = 2\n",
    "config.MODEL = 3\n",
    "config.OBJECTS = None #which objects?\n",
    "config.SEG_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config.INPUT_SIZE = 256 #input size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
